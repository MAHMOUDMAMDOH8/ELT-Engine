[2025-03-10T19:15:44.706+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-03-10T19:15:44.729+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: sales_pipeline.ingest_erp_data manual__2025-03-10T19:15:40.091817+00:00 [queued]>
[2025-03-10T19:15:44.738+0000] {taskinstance.py:2073} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: sales_pipeline.ingest_erp_data manual__2025-03-10T19:15:40.091817+00:00 [queued]>
[2025-03-10T19:15:44.738+0000] {taskinstance.py:2303} INFO - Starting attempt 1 of 1
[2025-03-10T19:15:44.751+0000] {taskinstance.py:2327} INFO - Executing <Task(PythonOperator): ingest_erp_data> on 2025-03-10 19:15:40.091817+00:00
[2025-03-10T19:15:44.760+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/***/task/task_runner/standard_task_runner.py:61: DeprecationWarning: This process (pid=85) is multi-threaded, use of fork() may lead to deadlocks in the child.
  pid = os.fork()

[2025-03-10T19:15:44.761+0000] {standard_task_runner.py:63} INFO - Started process 89 to run task
[2025-03-10T19:15:44.762+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'sales_pipeline', 'ingest_erp_data', 'manual__2025-03-10T19:15:40.091817+00:00', '--job-id', '66', '--raw', '--subdir', 'DAGS_FOLDER/pipeline.py', '--cfg-path', '/tmp/tmpbq09gjxl']
[2025-03-10T19:15:44.764+0000] {standard_task_runner.py:91} INFO - Job 66: Subtask ingest_erp_data
[2025-03-10T19:15:44.804+0000] {task_command.py:426} INFO - Running <TaskInstance: sales_pipeline.ingest_erp_data manual__2025-03-10T19:15:40.091817+00:00 [running]> on host 3c6b385b1ad0
[2025-03-10T19:15:44.894+0000] {taskinstance.py:2644} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Mahmoud' AIRFLOW_CTX_DAG_ID='sales_pipeline' AIRFLOW_CTX_TASK_ID='ingest_erp_data' AIRFLOW_CTX_EXECUTION_DATE='2025-03-10T19:15:40.091817+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-03-10T19:15:40.091817+00:00'
[2025-03-10T19:15:44.894+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-03-10T19:15:44.987+0000] {warnings.py:110} WARNING - /home/***/.local/lib/python3.12/site-packages/snowflake/connector/options.py:108: UserWarning: You have an incompatible version of 'pyarrow' installed (19.0.1), please install a version that adheres to: 'pyarrow<19.0.0; extra == "pandas"'
  warn_incompatible_dep(

[2025-03-10T19:15:45.088+0000] {connection.py:423} INFO - Snowflake Connector for Python Version: 3.14.0, Python Version: 3.12.2, Platform: Linux-6.11.0-19-generic-x86_64-with-glibc2.36
[2025-03-10T19:15:45.088+0000] {connection.py:1226} INFO - Connecting to GLOBAL Snowflake domain
[2025-03-10T19:15:45.089+0000] {connection.py:1315} INFO - This connection is in OCSP Fail Open Mode. TLS Certificates would be checked for validity and revocation status. Any other Certificate Revocation related exceptions or OCSP Responder failures would be disregarded in favor of connectivity.
[2025-03-10T19:15:46.004+0000] {snowflake_utilz.py:13} INFO - Connected to snowflake successfully
[2025-03-10T19:15:46.011+0000] {snowflake_utilz.py:26} INFO - Connected to PostgreSQL database successfully
[2025-03-10T19:15:46.011+0000] {pipeline.py:108} INFO - ðŸ“¥ Ingesting ERP data...
[2025-03-10T19:15:46.011+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-03-10T19:15:46.012+0000] {taskinstance.py:2890} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 235, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.12/site-packages/airflow/operators/python.py", line 252, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/airflow/dags/pipeline.py", line 110, in ingest_erp_data
    erp_cus = os.path.join(erp_path, "CUST_AZ12.csv")
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen posixpath>", line 76, in join
TypeError: expected str, bytes or os.PathLike object, not NoneType
[2025-03-10T19:15:46.022+0000] {taskinstance.py:1205} INFO - Marking task as FAILED. dag_id=sales_pipeline, task_id=ingest_erp_data, execution_date=20250310T191540, start_date=20250310T191544, end_date=20250310T191546
[2025-03-10T19:15:46.033+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 66 for task ingest_erp_data (expected str, bytes or os.PathLike object, not NoneType; 89)
[2025-03-10T19:15:46.058+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-03-10T19:15:46.074+0000] {taskinstance.py:3482} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-03-10T19:15:46.075+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
